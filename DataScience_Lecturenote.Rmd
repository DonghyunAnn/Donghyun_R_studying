---
title: "Datascience lecturenote"
author: "D H An"
date: '2021 1 20 '
output: html_document
---
Base Library
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:/Users/user/Documents/github_Rstudio_workspace/Data")
setwd("C:/Users/user/Documents/github_Rstudio_workspace/Data")

library("dplyr") #%>%
library("tidyr")
library("stringr")
library('rpart') # Decision Tree
library('rpart.plot') # Decision Tree
library('class') #Knearest
library('ROCR') #AUC 그래프
library('ggplot2') #시각화
```
## 데이터과학 2주차 강의노트
  
!(a>b) => 기존 TRUE FALSE값의 반대  
is.na(variable) => 결측값인주 물어보고 TRUE FALSE로 나옴  
typeof(variable) => 타입이 뭔지 알아보는거 / double=numeric  
  
### 벡터
```{r}
name_vector <- c('john', 'bob', 'sarah', 'alics')
name_vector[-1]
name_vector[c(2,4)]

some_vector <- c("John", "pocker player")
names(some_vector) <- c('name', 'job')
some_vector
some_vector[1]

weather_vector <- c("이름"="값", 'Mon'='sunny','Tues'='Rainy','Wed'='Cloudy','Thur'='Foggy',"Sat"='sunny',"Sun"="Cloudy")
names(weather_vector)
weather_vector[2]

a_vector <- 1:10 # seq(1,10)
b_vector <- seq(1,10,2)
c_vector <- rep(1:3,3)
d_vector <- rep(1:3,each=3)
c(a_vector,b_vector) # c(name_vector,a_vector)

a_vector <- c(1,5,2,7,8,2,3)
b_vector <- seq(1,10,3)
intersect(a_vector,b_vector) #교집합 union합집합 setdiff차집합 unique(중복제거) sum mean

#유용한 기능 
a_vector+10
a_vector>4
sum(a_vector>4)

#selection
sample_vector <- c(1,4,NA,2,1,NA,4,NA)
sample_vector[c(T,T,F,T,F,T,F,T)]
sample_vector
is.na(sample_vector)
sum(is.na(sample_vector))#na 몇개인지
na.omit(vector)# 결측값 제거
```


### matrix 행렬
```{r}
matrix(1:9, byrow=TRUE, nrow=3) # 열 방향으로 전개

new_hope <- c(460,314)
empire_strikes <- c(290,247)
return_jedi <- c(309,165)
star_wars_matrix <- matrix(c(new_hope,empire_strikes,return_jedi),nrow = 3,byrow=TRUE)
star_wars_matrix

region <- c('us','non us')
titles <- c('a new hope', 'the empire strikes back', 'return of the jedi')
colnames(star_wars_matrix) <- region
rownames(star_wars_matrix) <- titles
star_wars_matrix

rowSums(star_wars_matrix)#행 합
colSums(star_wars_matrix)#열 합

worldwide_vector <- rowSums(star_wars_matrix)
all_wars_matrix <- cbind(star_wars_matrix,worldwide_vector) #열끼리 묶음 열이추가
all_wars_matrix
apply(all_wars_matrix,2,mean)
box_office <- c(474,552,310,338,380,468)
star_wars_matrix2 <- matrix(box_office,nrow = 3, byrow = TRUE, 
                            dimnames = list(c('the phantom menace','attack of the clones', 'revenge ofthe sith'),c('us', 'nonus')))
star_wars_matrix2
rbind(star_wars_matrix,star_wars_matrix2) # 행끼리 묶음 행이 추가
#행렬 곱샘 %*% 각 성분끼리 곱은 *
```
factor변수
```{r}
sex_vector <- c("male","female","female","male","male") #type=character
factor_sex_vector <- factor(sex_vector) #type=factor 
#str() 변수의 구조를 보는 것임
typeof(factor_sex_vector)#integer인 이유는 수준이 1,2와 같이 나타나기 때문
str(factor_sex_vector)
levels(factor_sex_vector)
#Changing levels  
survey_vector <- c("m","f","f","m","m")
factor_survey_vector <- factor(survey_vector)
levels(factor_survey_vector) <- c('female','male')
```

### Dataframe & List
```{r}
#Dataframe
str(mtcars)
dim(mtcars)
head(mtcars)
mtcars$mpg >20
mtcars[mtcars$mpg>20,]#df[조건,(변수이름)]으로 select할 수 있음

#List
my_list <- list('a'=a_vector,'starwars'=star_wars_matrix,'cars'=mtcars)
names(my_list) <- c('a','starwars','car')
#정보 부르는법 my_list[[1]] 1자리에 name,혹은 my_list$starwars
```
## 데이터과학 3주차 강의노트  
  
### if문
if (condition) {exp}  
if (condition) {exp} else{exp}   
  
### for문
for(var in seq){exp}  
  
### ifelse문
ifelse(조건, 'true일때결과','False일때결과')


### DataLoading
getwd()
setwd("경로")
### Save & load
csv로 저장 -> write.csv(저장 대상, "저장할 파일 이름.csv", row.names = F or T,)
tsv로 저장 -> write.csv(저장 대상, "저장할 파일 이름.tsv", row.names = F or T, sep='\t')
  
Rdata로 저장 & 불러오기
save(벡터,벡터,다른거, file='파일이름.Rdata')
load('파일이름')

```{r}
#read.csv
getwd()
setwd("C:/Users/user/Documents/github_Rstudio_workspace/Data")
pools <- read.csv('swimming_pools.csv',stringsAsFactors = FALSE)
str(pools)
#read.table
hotdogs <- read.table('hotdog.txt',sep='\t',col.names = c('type','calories','sodium'))
hotdogs$cal.type <- ifelse(hotdogs$calories>150, 'heavy','light')#추가하고싶은변수그냥 냅다 설정하고 때려박으면됨
#Save
write.csv(hotdogs,file='newhotdogs.csv',row.names = F)
write.table(hotdogs,file='newhotdogs.tsv',row.names = F, sep='\t')
```

### 유용한 함수 사용
사용자 정의 함수 함수명 <- function(변수){return(출력값)}  
runif(개수) ->0~1 에 대해 난수생성하는것(set.seed(2018)등 숫자설정하면 난수가 똑같이나옴)  
apply(x,margin,function) margin -> 1=row, 2=column 즉 dataframe을 슬라이스내서 그걸로 함수적용하는것
* lapply(x,function) -> listapply라고 보면됨
* sapply(x,function) -> 결과가 벡터나 행렬로 나옴  #sapply(x,function(x){return값}) -> 익명 함수
* tapply는 행렬 기준임 tapply(다룰 백터, 그룹백터 혹은 그룹으로 나눌만한 조건, function)
* aggregate(종속변수~독립변수, data=어떤데이터, function)
* sort(백터, decreasing=T)
* order(백터) 결과는 정렬되는 인덱스 백터가 나옴 **sort는 order을 내포하고 있음**
* 즉 order는 기준을 설정하고 데이터 프레임 내 정렬할 때 쓰기 좋음
* sample(x,샘플개수,replace=False) 
* split(df, split_var) ->데이터 프레임을 어떤 기준으로 쪼개서 리스트로 나타내주는 것
* subset(dataframe, 조건)
* merge(df1,df2)
* which(조건) ->조건을 충족한 인덱스를 찾는것 위치를 찾는것
* cut(vector, breaks = c(0,2,4,6), label=c(1,2,3),right) -> 명목형 변수로 나타낼 때
* quantile->기본이 4분위수 하지만 probs로 내가 원하는 퍼센테이지 볼 수 있음
* paste 1. 문자끼리 붙이기 2. 벡터 인덱스 같은것끼리 붙이기
```{r}
#apply
tapply(iris$Sepal.Length, iris$Species, mean)
tapply(mtcars$wt,mtcars$mpg>20, mean)
#sort,order
sort(mtcars$mpg)
order(mtcars$mpg)
mtcars[order(mtcars$mpg, decreasing=T),]
iris[order(iris$Petal.Length, decreasing=T),]
sample(1:150, 10,replace = FALSE)
iris[sample(1:150, 10,replace = FALSE),]
sample(a_vector,length(a_vector))#순서섞는것도 가능
#split
split(mtcars,mtcars$cyl)
split(iris,iris$Petal.Width>2)
#merge
x <- data.frame(name=c("john",'bob','carol'),
                math=c(70,80,90))
y <- data.frame(name=c('john','bob','alice'),
                history=c(100,80,90))
merge(x,y)
merge(x,y,all=T)
#cut
mtcars$wt
cut(mtcars$wt,breaks = c(0,2,4,6), label=c(1,2,3), right=TRUE)
#quantile
quantile(iris$Sepal.Length, probs = c(0.1,0.9))
cutpoint <- quantile(mtcars$mpg,probs=c(0,0.25,0.75,1))
mtcars$fuelefficiency <- cut(mtcars$mpg,breaks=cutpoint,include.lowest = T)
#paste
x <- seq(2,20.2)
y <- LETTERS[1:10]
paste(x,y,sep = ';')
paste(paste(x,y), collapse = ',')
outcome <- 'mpg'
input_var <- names(mtcars)[2:6]
paste(outcome,paste(input_var,collapse='+'),sep = '~')
```
## 데이터과학4주차 강의노트
### Explore Data
```{r}
bmi<- read.csv("C:/Users/user/Documents/github_Rstudio_workspace/Data/bmi_clean.csv",header = TRUE)
str(bmi)
dim(bmi)
names(bmi)
class(bmi)
summary(bmi)
glimpse(bmi)
#시각화
hist(bmi$Y1980)
plot(x=bmi$Y1980,y=bmi$Y2008)
```


### Data Clearing
* gather(df, columnname,columnval,columnname 범위) # 넓은 데이터를 좁게
* spread(data, 쪼갤column) #근데 이걸 구분시켜줄 어떤 변수가 있어야함
* separate(df, col, c(변수,변수, ...),sep='-'는 디폴트) 변수를 나눠주는 것 
* unite(df,변수이름, col, col)

```{r}
wide_df <- data.frame(col = c('X', 'Y'), A = c(1,4), B = c(2,5), C = c(3,6))
long_df <- gather(wide_df,my_key, my_val, 2:4)
spread(long_df,my_key,my_val)
#exercise01
bmi_long <- gather(bmi, year, bmi_val, 2:30)
#bmi_long
bmi_wide <- spread(bmi_long, year,bmi_val)
#bmi_wide
#exercise02
bmic <- read.csv('C:/Users/user/Documents/github_Rstudio_workspace/Data/bmi_cc.csv', header=TRUE)
head(bmic)
bmic_clean <- separate(bmic,Country_ISO,c('Country','ISO'),sep='/')
#unite(bmic_clean,Country_ISO,Country,ISO, sep='/')
```

### Type Conversion 
* class()
* as.logical()
* as.numeric()
* library('lubridate') => ymd, mdy <-  date, hms <- time
```{r}
date <- c('2010-10-01','2010-12-31')
summary(as.Date(date))
```
### String Manipulation
* str_trim() :좌우 공백제거
* str_pad(변수, with=숫자, side='left', pad='0') :자리수맞춰주기
* str_detect(vector,'찾고싶은거')
* str_replace(vector,'빼고싶은거','넣고싶은거')
* tolower() : 소문자
* toupper() : 대문자
```{r}
#exercise3
students2 <- read.csv('C:/Users/user/Documents/github_Rstudio_workspace/Data/students2.csv',header=TRUE,stringsAsFactors = F)
str(students2)
library('lubridate')
students2$dob <- ymd(students2$dob)
students2$nurse_visit <- ymd_hms(students2$nurse_visit)
str(students2)

#exercise4
str_detect(students2$dob,'1997')
students2$sex <- str_replace(students2$sex,'F','Female')
students2$sex <- str_replace(students2$sex,'M','Male')
head(students2)
```

### NA Handling
* is.na(df) : TRUE FALSE로 결과치 나옴
* any(is.na(df)) : 결측치가 있냐
* sum(is.na(df)) : 개수새기
* complete.cases(df) : 결측치가 하나도 없는 행=TRUE
* df[complete.cases(df),] : na가 없는 행
* na.omit(df) : na행 다 날려버리기기  

```{r}
students3 <- read.csv('C:/Users/user/Documents/github_Rstudio_workspace/Data/students3.csv',header=TRUE,stringsAsFactors = F)
str(students3)
attach(students3)
summary(age)
summary(absences)
hist(age)
hist(absences)
students3$absences <- str_replace(absences,'-1','NA')
#students3
str_detect(absences,'NA')
```
## 데이터과학 9주차 강의노트
* precision=TP/TP+FP(2종오류)(참이라고 예측한거) |  recall=TP/TP+FN(1종오류)(사실이 참인거)
* threshold가 증가 -> precision증가 recall 감소 | threshold 감소 ->precision감소 recall증가 
* AUC curve-> threshold와 관계없이 모델의 기능을 알 수 있는 법(여러가지 threshold를 넣어서 그 점들을 이어서 만드는 것이다)
* True positive rate=전체 actual참 값 중에  True 인 값 | False Positive rate=전체 actual 거짓 값 중에 못 찾아낸 비율
* TPR=TP/TP+FN FPR=FP/FP+FN 
* 0.9~1 -> excellent\0.8~0.9->good\00.7~0.8 -> fair

### 명목형 변수로 model만들기
```{r}
#Data load
load(url('https://github.com/hbchoi/SampleData/raw/master/adult.RData'))
set.seed(2020)
#train, test set 설정
n_sample <- nrow(adult)
rgroup <- runif(n_sample)#균등분포에서의 난수생성
adult.train <- subset(adult,rgroup<=0.8)
adult.test <- subset(adult, rgroup>0.8)
#table 쓰는 법
table(adult.train$income_mt_50k)#명목형 변수 빈도를 보여줌
prop.table(table(adult.train$income_mt_50k))#소수점 비율로 보여줌
# 모델 전 처리
tble <- table(adult.train$occupation,adult.train$income_mt_50k)#row,column
sv_model_job <- prop.table(tble,margin = 1)[,2] # 뒤에 [,] 붙여서 원하는 행 열 뽑아낼 수 있음
adult.train$est_prob <- sv_model_job[adult.train$occupation]
#간단한 모델 만들기
threshold <- 0.4
adult.train$prediction <- adult.train$est_prob>threshold
head(adult.train[,c('occupation','est_prob','prediction','income_mt_50k')],10)
#얼마나 맞췄는지 확인
conf.table <- table(pred=adult.train$prediction, actual=adult.train$income_mt_50k)
conf.table
accuracy <- sum(diag(conf.table))/sum(conf.table)
accuracy
#이후에는 train set에서 같은 기준을 적용했을 때 어느 정도 나오는지를 확인해야 함
```
####명목형 변수로 model만들기 exercise
```{r}
# 모델 전 처리
tble <- table(adult.train$education,adult.train$income_mt_50k)
sv_model_job <- prop.table(tble,margin = 1)[,2]
adult.train$est_prob <- sv_model_job[adult.train$education]
#역치0.5
threshold <- 0.5
adult.train$prediction <- adult.train$est_prob>threshold
conf.table <- table(pred <- adult.train$prediction,actual <- adult.train$income_mt_50k)
accuracy <- sum(diag(conf.table))/sum(conf.table)
accuracy
recall <-  sum(conf.table[2,2])/sum(conf.table[,2])
recall
precision <- sum(conf.table[2,2])/sum(conf.table[2,])
precision
#역치0.6
threshold <- 0.6
adult.train$prediction <- adult.train$est_prob>threshold
conf.table <- table(pred <- adult.train$prediction,actual <- adult.train$income_mt_50k)
accuracy <- sum(diag(conf.table))/sum(conf.table)
accuracy
recall <-  sum(conf.table[2,2])/sum(conf.table[,2])
recall
precision <- sum(conf.table[2,2])/sum(conf.table[2,])
precision
#test set 기준으로 얼마나 맞았는지 확인
tble <- table(adult.test$education,adult.test$income_mt_50k)
sv_model_job <- prop.table(tble,margin = 1)[,2]
adult.test$est_prob <- sv_model_job[adult.test$education]
threshold <- 0.5
adult.test$prediction <- adult.test$est_prob>threshold
conf.table <- table(actual <- adult.test$prediction, actual <- adult.test$income_mt_50k)
accuracy <- sum(diag(conf.table))/sum(conf.table)
accuracy
#AUC curve Visualization
plot(performance(prediction(adult.test$est_prob,adult.test$income_mt_50k),'tpr','fpr')) #앞변수로 뒷변수를 얼마나 잘 예측하 수 있느냐

```

####연속형을 명목형으로 바꿔서 분석 exercise
```{r}
adult.train$age_group <- cut(adult.train$age, breaks=c(0,20,30,40,50,60, Inf),labels=c('under20','20s','30s','40s','50s','over60'), 
                             right = F)
tble <- table(adult.train$age_group,adult.train$income_mt_50k)
sv_model_age <- prop.table(tble,margin=1)[,2]
adult.train$est_prob <- sv_model_age[adult.train$age_group]
threshold <- 0.3
adult.train$predictoni <- adult.train$est_prob >threshold
#accuracy 사용자 정의 함수
get_accuracy <- function(pred,actual){
  tble <- table(pred,actual)
  return(round(sum(diag(tble))/sum(tble),3))
}
get_accuracy(adult.train$prediction,adult.train$income_mt_50k)
```
###연속형 변수 model 만들기 회귀분석
* mse와 rmse로 모형이 정확한지 알 수 있음 이건 log를 씌웠으니 그 점까지 고려해야함
* 표준편차보다 더 작아야 의미가 있다
* 회귀 분석 생각해보면 SST(y_i-E(y))^2=SSreg(E(y)-yhat)^2+SSE(y_i-yhat)^

```{r}
#Data load
load(url('https://github.com/hbchoi/SampleData/raw/master/insurance.RData'))
#Data Browse
hist(insurance$charges)
plot(density(log(insurance$charges)))#log를 씌우면 편포가 정규모양으로 바뀔 수 있음
insurance$logcharges <- log10(insurance$charges)
#test, training set
set.seed(2020)
n_sample <- nrow(insurance)
rgroup <- runif(n_sample)
train <- subset(insurance,rgroup<=0.8)
test <- subset(insurance,rgroup>0.8)
#model 전처리
sv_reg_smoker <- tapply(train$logcharges, train$smoker,mean)
train$pred <- sv_reg_smoker[train$smoker]
train$error <- train$logcharges-train$pred
msetrain <- mean(train$error**2)
msetrain
sqrt(msetrain)
```

## 데이터과학 11주차 강의노트
### Decision Tree
* 대출클럽같은 곳에 많이 쓰임
* Entropy = 혼잡도 : 데이터가 비슷하게 섞여있으면 높고, 한가지 종류만 있으면 낮음
* {sigma_(k- > 분할개수) -p_k*logp_k} {p_k=전체중에 해당 부분}
  
* root= 전체, split=나눈거 , n=샘플수, loss=rapid가 아닌 수(deafult, rapid)
* 해석방법: root 전체개수 나눈개수 구하고자하는것(x) (1-p(x),p(x))
* prepruning은 미리 복잡함의 정도를 정해두는 것임 [maxdepth ,minimumsplit (쪼개진 샘플 수가 일정 수준 이하로 내려가지 않게 하는 것)]
* postpruning은 다 만들고 어느 선에서 정리하는 것 [plotcp(model)쓰면 model의 에러값과 cp의 값 plotd이 뜨는데 여기서 cp값 보고 결정]
* predict(model,data,type='class' 등등)
* cp=0을 둔다는 것은 제한없이 쭉 끝까지 한다는 뜻
```{r}

load(url('https://github.com/hbchoi/SampleData/blob/master/dtree_data.RData?raw=true'))
str(loans)
loan_model <- rpart(outcome~loan_amount+credit_score,data=loans,method='class',
                    control=rpart.control(cp=0))
loan_model
rpart.plot(loan_model)
rpart.plot(loan_model, type=3, box.palette=c('red','green'),fallen.leaves= TRUE)


#Prepruning maxdepth로 정하기
loan_model <- rpart(outcome ~ ., data=loans, method='class',
                    control=rpart.control(cp=0,maxdepth=6))
#Prepruning minimumsplit으로 정하기
loan_model <- rpart(outcome ~ ., data=loans, method='class',
                    control=rpart.control(cp=0,minsplit = 500))
#Postpruning
loan_model_pruned <- prune(loan_model,cp=0.0014) # 이런 식으로 결정
```
###K Nearest Neighbors
* pred <- knn(train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)
- train : 훈련 데이터셋 (matrix or data frame)
- test : 테스트 데이터셋 (matrix or data frame)
- cl: 훈련 데이터셋의 그룹(class) 정보 (factor)
- k : 이웃(neighbour)의 수
- l : 명확한 결정을 위한 최소 유효 수, 그렇지 않은 경우 의심
- prob: 만약 이것이 트루라면, 프로브 속성에서 당첨된 비율을 반환한다.
- use.all: 만약 트루라면, K번째의 최대치에 해당하는 모든 거리가 포함된다. 거짓이면 k의 이웃 포인트를 정확히 사용하기 위해 K번째와 같은 거리를 무작위로 선택한다.
* 유클리디안 거리를 구하기 위해 표준화를 시켜야 함

```{r}
wbcd <- read.csv("https://github.com/hbchoi/SampleData/raw/master/wisc_bc_data.csv",stringsAsFactors=F)
wbcd <- wbcd[,-1]
wbcd$diagnosis <- ifelse(wbcd$diagnosis=='B',' Benign','Malignant')
table(wbcd$diagnosis)

#min-max normalization for Euclidean Distance
minmax_norm <- function(x){
  (x-min(x))/(max(x)-min(x))
}
wbcd_norm <- sapply(wbcd[-1], minmax_norm)
summary(wbcd)
summary(wbcd_norm)
#train test
wbcd_train <- wbcd_norm[1:469,]
wbcd_test <- wbcd_norm[470:569,]
wbcd_train_label <- wbcd[1:469,1]
wbcd_test_label <- wbcd[470:569,1]

# k개수는 총 행 개수의 루트로 구했음
sqrt(nrow(wbcd_train))
wbcd_test_pred <- knn(train=wbcd_train,test=wbcd_test,cl=wbcd_train_label,k=21)
#accuracy
mean(wbcd_test_label==wbcd_test_pred)
#confusion matrix
cmat <- table(wbcd_test_label,wbcd_test_pred)
cmat
precision <- cmat[2,2]/sum(cmat[,2])
recall <- cmat[2,2]/sum(cmat[2,])
#암인사람 주변이 암인사람 비율, 암이 아닌사람 주위 암이 아닌사람 비율
wbcd_test_pred <- knn(train=wbcd_train,test=wbcd_test,cl=wbcd_train_label,k=21,prob=TRUE)
head(attributes(wbcd_test_pred)$prob)
head(wbcd_test_pred)
wbcd_test_pred_prob <-  ifelse(wbcd_test_pred == 'M', attributes(wbcd_test_pred)$prob, 1-attributes(wbcd_test_pred)$prob)
head(wbcd_test_pred_prob)
```
###시각화 AUC
```{r}
library('ROCR')
plot(performance(prediction(wbcd_test_pred_prob,wbcd_test_label=='Malignant'),'tpr','fpr'))
calAUC <- function(predCol,targetCol){
  perf <- performance(prediction(predCol,targetCol),'auc')
  as.numeric(perf@y.values)
}

calAUC(wbcd_test_pred_prob,wbcd_test_label=='Malignant')
performance(prediction(wbcd_test_pred_prob,wbcd_test_label=='Malignant'),'auc')

#adjusting threshold
threshold <- 0.1
wbcd_test_pred_new <- ifelse(wbcd_test_pred_prob > threshold, 'Malignant','Benign')
cmat <- table(wbcd_test_label, wbcd_test_pred_new)

#accuracy
mean(wbcd_test_label == wbcd_test_pred_new)
precision <- cmat[2,2]/sum(cmat[,2])
recall <- cmat[2,2]/sum(cmat[2,])
precision
recall
```

###더미변수
* one hot encoding
* library(caret)
* predict(dummyVars(~cbwd,data=data1),data1)

## 데이터과학 13주차 강의노트
###linear model
* model <- lm(outcome~independent,data=train)
* prediction <- predict(model,test)
* 명목형 변수들은 자동으로 더미 변환이되어 계산되는데, #factor개수 -1 로 진행됨 (000 100 010 001 이렇게)
* 명목형변수 level 2인 경우 알파벳 순으로 먼저 오는게 reference 값
```{r}
#data load
load(url('https://github.com/hbchoi/SampleData/raw/master/insurance.RData'))
str(insurance)
summary(insurance)
#test train set
rgroup <- runif(nrow(insurance))
train <- subset(insurance, rgroup<=0.8)
test <- subset(insurance,rgroup>0.8)
#model & predict
ins_model <- lm(charges~age + sex + bmi+children + smoker + region,train)
train$pred <- predict(ins_model,train)
test$pred <- predict(ins_model,test)
#function
calcrmse <- function(real,estimation){
  return(sqrt(mean((real-estimation)**2)))
}
calr2 <- function(real,estimation){
  rss <- sum((real-estimation)**2)
  sst <- sum((real-mean(real))**2)
  return(1-rss/sst)
}

calcrmse(train$charges,train$pred)
calr2(train$charges,train$pred)
calcrmse(test$charges,test$pred)
calr2(test$charges,test$pred)
summary(ins_model)
```
###시각화
```{r}
ggplot(train,aes(x=pred, y=charges))+
  geom_point(alpha=0.2, col='black')+
  geom_smooth()+
  geom_line(aes(x=charges,y=charges),col='blue',linetype=2)

ggplot(train,aes(x=pred,y=pred-charges))+ 
  geom_point(alpha=0.2,col='black')+
  geom_smooth()+
  geom_hline(yintercept=0,col='blue',linetype=2)
```
#### 모형 개선을 위해 다항 회귀를 할 수 도 있고, 교호작용항을 넣을 수도 있다.
* lm(outcome~va1 * var2) 이렇게 설정 시 var1, var2,var1*var2 다들어감
```{r}
#모형개선
train$bmi30 <- ifelse(train$bmi>=30,1,0)
test$bmi30 <- ifelse(test$bmi>=30,1,0)
ins_model <- lm(charges~age+I(age^2)+sex+bmi+children+bmi30*smoker+region, train)
train$pred <- predict(ins_model,train)
test$pred <- predict(ins_model,test)

calcrmse(train$charges,train$pred)
calr2(train$charges,train$pred)
calcrmse(test$charges,test$pred)
calr2(test$charges,test$pred)
summary(ins_model)
```

###logistic medel
* log(odds)= log(p/(1-p))
* log(p/(1-p))= ax + b -> p = 1/(1+e^(-(ax+b)))
* 회귀식 ax+b 가 0보다 크냐 작으냐에 따라 달라진다
```{r}
#data load
load(url('https://github.com/hbchoi/SampleData/raw/master/NatalRiskData.rData'))
str(sdata)
#train test set
train <- sdata[sdata$ORIGRANDGROUP<=5,]
test <- sdata[sdata$ORIGRANDGROUP>5,]
#variable setting
complications <- c('ULD_MECO','ULD_PRECIP','ULD_BREECH')
riskfactors <- c('URF_DIAB','URF_CHYPER','URF_PHYPER','URF_ECLAM')
y <- 'atRisk'
x <- c('PWGT','UPREVIS','CIG_REC','GESTREC3','DPLURAL',complications,riskfactors)
fmla <- paste(y,paste(x,collapse='+'),sep='~')
fmla
#logistic model
model <- glm(fmla,data=train,family=binomial(link='logit'))
train$pred <- predict(model,newdata=train,type='response') #newdata=예측하고자하는 데이터
test$pred <-predict(model,newdata=test,type='response') #type='response'로 해주어야 확률값이 나옴
#예측된 값 보기
test[20:40,c('pred','atRisk')]
aggregate(pred~atRisk,data=train,mean)
aggregate(pred~atRisk,data=test,mean)
ggplot(train, aes(x=pred,color=atRisk,linetype=atRisk))+geom_density()
#threshold에 대한 pnull-> 전체에 적용했을 때 확률, 1000명 전체에 응급대처했을 때 20명0.02, -> 그래서 threshold를 적용해서 예측했을 때 맞을 확률이 얼마나 높은지 확인하는거
#p/pnull=enrich
```

## 데이터과학 15주차 강의노트
###군집분석
* unsupervised method는 명확한 판단기준은 없음
* Total within sum of square(wss): centroid와 군집내 변수들간의 거리 제곱의 평균, 작을 수록 좋다. cluster 개수가 많을 수록 값이 낮아지는 한계점이 있음
* Calinski-Harabasz index: (Bss(k)/k-1)/(wss(k)/n-k) |Between sum of square(bss)= tss-wss
```{r}
#데이터셋 만들기
set.seed(2020)
synth.data<-data.frame(x1 =c(rnorm(20, 3, 1.5), rnorm(20,0,1), rnorm(20,5,1)), 
                       x2 =c(rnorm(20, 0, 1), rnorm(20,4,1), rnorm(20,5,1)))
ndata<-nrow(synth.data)#행개수
ndim<-ncol(synth.data)#열개수
synth.data%>%ggplot(aes(x =x1, y=x2)) +geom_point(shape =1) +theme_bw()
#유클리디안거리 함수
u_dist<-function(u, v){sqrt(sum((u -v) **2))}
#초기 central 값
set.seed(2020)
k <-3
cents <-data.frame(cl=1:k)
cents <-cbind(cents, synth.data[sample(1:60, k),])

#군집분석 plot
synth.data$cl<-factor(rep(1, ndata), levels =1:k)
synth.data%>%ggplot(aes(x =x1, y=x2, col =cl)) +geom_point(shape =1) +theme_bw() +geom_point(data =cents, shape =4, col ='red')

#군집분석 반복문
while(TRUE){
  # data assignment to cluster
  new_cl<-apply(synth.data[,1:ndim], 1, function(x) {
    which.min(
      apply(cents[,-1], 1, function(y) {
        u_dist(y, x)
      })
    )
  })
  if(all(synth.data$cl==factor(new_cl))) break
  
  synth.data$cl<-factor(new_cl)
  cents <-synth.data%>%group_by(cl)%>%
    summarise(x1 =mean(x1), x2 =mean(x2)) 
}

#군집분석 전처리
protein <-read.table("C:/Users/user/Documents/github_Rstudio_workspace/Data/protein.txt", sep="\t", header=TRUE)
vars.to.use<-colnames(protein)[-1] #나라 없애기
pmatrix<-scale(protein[,vars.to.use]) #  표준화
pcenter<-attr(pmatrix, "scaled:center") # 평균 저장
pscale<-attr(pmatrix, "scaled:scale") #표준편차 저장
#군집분석
pclusters<-kmeans(pmatrix, 5, nstart=100, iter.max=100) # 2번쨰=k값, 100번반복하겠다
summary(pclusters)
pclusters$centers # 각각 군집의 편균값 어떤 특징인지 볼 수 있음
pclusters$size # 각 군집당 몇개인가
#지표 추출
pclusters$withinss
(wss<-sum(pclusters$withinss))
(tss<-pclusters$totss)
(bss<-pclusters$betweenss)
ch.index<-(bss/(k-1)) /(wss/(ndata-k))
print(sprintf('CH index of this clustering is %.f', ch.index))
```